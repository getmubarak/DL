{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of NN](https://mashimo.files.wordpress.com/2017/01/neuronxor.png?w=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Simulation of XOR gate using ANN ***\n",
      "***\n",
      "** converged after iterations:  292\n",
      "[[ 1.07917645  2.57719349]\n",
      " [-4.51281706 -1.96060129]\n",
      " [-4.56064529 -1.98098059]]\n",
      "[[-1.17170874]\n",
      " [-5.14756726]\n",
      " [ 3.65391918]]\n",
      "X1 | X2 | Y = X1 XOR X2\n",
      "------------------------\n",
      "0  |  0  |  0\n",
      "0  |  1  |  1\n",
      "1  |  0  |  1\n",
      "1  |  1  |  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):  \n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoidDeriv(s):\n",
    "  return s * (1-s)\n",
    "\n",
    "def sigmoidActivation(z):\n",
    "  if sigmoid(z) > 0.5:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def learnWeightsTwoLayers(X, y, tolerance = 0.1, max_iterations = 1000):\n",
    "  \"\"\"\n",
    "  Learn the weights for an artificial neural network with two layers,\n",
    "  given a training set of input and output values.\n",
    "  Uses a backpropagation algorithm.\n",
    "\n",
    "  Arguments:\n",
    "    X: input data points without bias, matrix\n",
    "    y: output data points, array\n",
    "    tolerance: minimum error to stop the training, float. Optional (default=0.1)\n",
    "    max_iterations: maximum number of iterations performed by the algorithm.\n",
    "                    Optional. Default=1000\n",
    "\n",
    "  Returns:\n",
    "    weights for input layer, array\n",
    "    weights for hidden layer, array\n",
    "  \"\"\"  \n",
    "\n",
    "    # input and output training sizes\n",
    "  n_inputs = X.shape[1]\n",
    "  n_outputs = y.shape[1]\n",
    "  n_examples = X.shape[0]\n",
    " \n",
    "    # add bias\n",
    "  bias = np.ones(n_examples)\n",
    "  X = np.c_[bias, X] # add bias column to X\n",
    " \n",
    "    # initialize weights randomly with mean 0\n",
    "  w1 = 2 * np.random.random((n_inputs+1, n_inputs)) - 1 # input layer\n",
    "  w2 = 2 * np.random.random((n_inputs+1, n_outputs)) - 1 # hidden layer\n",
    " \n",
    "   # initialization\n",
    "  converged = False\n",
    "  i = 0 # iteration counter\n",
    "\n",
    "   # main loop until converged or max iterations reached\n",
    "  while not converged and i <= max_iterations:\n",
    " \n",
    "    # note: all training examples together (batch)\n",
    " \n",
    "       # forward propagation\n",
    "    hiddenLayer = sigmoid(np.dot(X, w1)) \n",
    "    hiddenLayer = np.c_[bias, hiddenLayer] # add bias \n",
    "    pred_y = sigmoid(np.dot(hiddenLayer, w2))  # predicted output\n",
    " \n",
    " \n",
    "       # Compute the error\n",
    "    error_y = y - pred_y \n",
    " \n",
    "       # backpropagation: multiply how much we missed by the\n",
    "       # slope of the sigmoid at the values in y_pred\n",
    "    delta_y = error_y * sigmoidDeriv(pred_y)\n",
    " \n",
    "       # repeat for hidden layer: error and delta\n",
    "       # how much did each value of hidden layer contribute \n",
    "       # to the final error (according to the weights)?\n",
    "    error_l1 = delta_y.dot(w2.T)\n",
    "    delta_l1 = error_l1 * sigmoidDeriv(hiddenLayer)\n",
    "       # the first column refers to the bias: remove it\n",
    "    delta_l1 = np.delete(delta_l1, 0, axis=1)\n",
    " \n",
    "      # update weights\n",
    "    w2 += np.dot(hiddenLayer.T, delta_y) \n",
    "    w1 += np.dot(X.T, delta_l1) \n",
    "\n",
    "      # did we converge?\n",
    "    maxError = max(abs(error_y))\n",
    "    if abs(maxError) < tolerance:\n",
    "      converged = True # yes !\n",
    "      print (\"** converged after iterations: \",i)\n",
    "\n",
    "    i += 1 # next iteration (to check if max is reached)\n",
    "\n",
    "  if not converged:\n",
    "    print(\"** Not converged!\")\n",
    " \n",
    "  return w1, w2\n",
    "\n",
    "def predictTwoLayers(X, w1, w2):\n",
    "  \"\"\"\n",
    "  Compute the binary output of an Artificial Neural Network with two layers,\n",
    "  given its neuron weights and an input dataset\n",
    "\n",
    "  Arguments:\n",
    "    X: input data points without bias, list\n",
    "    w1: the input-layer weights, array\n",
    "    w2: the hidden-layer weights, array\n",
    "\n",
    "  Returns:\n",
    "    integer (0 or 1)\n",
    "  \"\"\" \n",
    "  bias = np.ones(1)\n",
    "  X = np.append(bias, X, 0) # add bias column to X\n",
    "\n",
    "    # forward propagation\n",
    "  hiddenLayer = sigmoid(np.dot(X, w1)) \n",
    "  hiddenLayer = np.append(bias, hiddenLayer, 0) # add bias \n",
    "  y = np.dot(hiddenLayer, w2) \n",
    "\n",
    "  return sigmoidActivation(y)\n",
    "\n",
    "print(\"*** Simulation of XOR gate using ANN ***\")\n",
    "print(\"***\")\n",
    "\n",
    "  # input dataset\n",
    "X_train = np.array([ [0,0], [0,1], [1,0], [1,1] ])\n",
    "  # output dataset \n",
    "y_train = np.array([[0,1,1,0]]).T\n",
    "   # note that we use tolerance = 0.4\n",
    "XORweights1, XORweights2 = learnWeightsTwoLayers(X_train, y_train, 0.4)\n",
    "\n",
    "print (XORweights1)\n",
    "print (XORweights2)\n",
    "\n",
    "print(\"X1 | X2 | Y = X1 XOR X2\")\n",
    "print(\"------------------------\")\n",
    "for x1 in (0,1):\n",
    "  for x2 in (0,1):\n",
    "    print(x1,\" | \",x2, \" | \", predictTwoLayers(np.array([x1,x2]),XORweights1, XORweights2))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.google.co.in/url?sa=t&rct=j&q=&esrc=s&source=web&cd=4&ved=0ahUKEwiz1dmtyL3XAhUCkpQKHdAEBnoQFgg4MAM&url=https%3A%2F%2Fmattmazur.com%2F2015%2F03%2F17%2Fa-step-by-step-backpropagation-example%2F&usg=AOvVaw00V9hBpzEFXJTayo3kxF2P\n",
    "\n",
    "https://mashimo.wordpress.com/2015/09/13/back-propagation-for-neural-network/\n",
    "\n",
    "https://stevenmiller888.github.io/mind-how-to-build-a-neural-network/\n",
    "\n",
    "https://iamtrask.github.io/2015/07/27/python-network-part2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
