{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "def log(x):\n",
    "    return 1 / ( 1+ np.exp( -1 * x ))\n",
    "\n",
    "def d_log(x):\n",
    "    return log(x) * (1 - log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learn(input,output,number_of_epoch):\n",
    "    state = np.zeros((x.shape[0],input.shape[1] + 1))\n",
    "    wx = np.random.randn()\n",
    "    wrec = np.random.randn()\n",
    "    lr_wx = 0.001\n",
    "    lr_wrec = 0.001\n",
    "    grad_over_time = np.zeros((input.shape))\n",
    "    \n",
    "    for iter in range(number_of_epoch):\n",
    "        state_1_in  = state[:,0]*wrec + input[:,0]*wx\n",
    "        state_1_out = log(state_1_in)\n",
    "        state[:,1] = state_1_out\n",
    "\n",
    "        state_2_in  = state[:,1]*wrec + input[:,1]*wx\n",
    "        state_2_out = log(state_2_in)\n",
    "        state[:,2] = state_2_out\n",
    "\n",
    "        state_3_in  = state[:,2]*wrec + input[:,2]*wx\n",
    "        state[:,3] = state_3_in\n",
    "\n",
    "        cost = np.square(state[:,3] - np.squeeze(y)).sum() / len(input)\n",
    "\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iter : \", iter, \" Current cost: \", cost)\n",
    "\n",
    "        grad_over_time[:,2] = (state[:,3] - np.squeeze(output)) * (2/len(input))\n",
    "        grad_over_time[:,1] = grad_over_time[:,2] * wrec  * d_log(state_2_in)\n",
    "        grad_over_time[:,0] = grad_over_time[:,1] * wrec  * d_log(state_1_in)\n",
    "\n",
    "        grad_wx = np.sum(grad_over_time[:,2]*input[:,2]+\n",
    "                    grad_over_time[:,1]*input[:,1]+\n",
    "                    grad_over_time[:,0]*input[:,0])\n",
    "\n",
    "        grad_wrec = np.sum(grad_over_time[:,2]*state[:,2]+\n",
    "                    grad_over_time[:,1]*state[:,1]+\n",
    "                    grad_over_time[:,0]*state[:,0])\n",
    "\n",
    "        wx = wx - lr_wx * grad_wx\n",
    "        wrec = wrec - lr_wrec * grad_wrec\n",
    "    \n",
    "    return wx,wrec\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Normally, the state of your hidden layer is based ONLY on your input data. So, normally a neural network's information flow would look like this:\n",
    "input -> hidden -> output\n",
    "\n",
    "In Neural Memory, the hidden layer is a combination of your input data at the current timestep and the hidden layer of the previous timestep.\n",
    "\n",
    "(input + prev_hidden) -> hidden -> output\n",
    "\n",
    "Here, we have 4 timesteps of a recurrent neural network pulling information from the previous hidden layer.\n",
    "\n",
    "(input + empty_hidden) -> hidden -> output\n",
    "(input + prev_hidden) -> hidden -> output\n",
    "(input + prev_hidden) -> hidden -> output\n",
    "(input + prev_hidden) -> hidden -> output\n",
    "\n",
    "\n",
    "Why the hidden layer? Well, we could technically do this.\n",
    "\n",
    "(input + prev_input) -> hidden -> output\n",
    "\n",
    "In the hidden layer recurrence, we see a presence of every input seen so far. In the input layer recurrence, it's exclusively defined by the current and previous inputs. Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint. Think about it, if the song had the statements \"I love you\", and \"I love carrots\", and the network was trying to predict the next word, how would it know what follows \"I love\"? It could be carrots. It could be you. The network REALLY needs to know more about what part of the song its in. However, the \"hidden layer recurrence\" doesn't break down in this way. It subtely remembers everything it saw (with memories becoming more subtle as it they fade into the past). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model(input):\n",
    "    state = np.zeros((input.shape[0],input.shape[1] +1))\n",
    "    \n",
    "    state_1_in  = state[:,0]*wrec + input[:,0]*wx\n",
    "    state_1_out = log(state_1_in)\n",
    "    state[:,1] = state_1_out\n",
    "\n",
    "    state_2_in  = state[:,1]*wrec + input[:,1]*wx\n",
    "    state_2_out = log(state_2_in)\n",
    "    state[:,2] = state_2_out\n",
    "\n",
    "    state_3_in  = state[:,2]*wrec + input[:,2]*wx\n",
    "    state[:,3] = state_3_in\n",
    "    return state_3_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "\n",
      "Final output Raw:  [-0.42324499  0.04819017 -0.08716405]\n",
      "Final output Rounded:  [-0.  0. -0.]\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = np.array([\n",
    "    [0,0,0],\n",
    "    [0,0,1],\n",
    "    [0,1,1]\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    [3],\n",
    "    [2],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "wx, wrec=np.random.randn(),np.random.randn()\n",
    "\n",
    "# 3. Final Output\n",
    "result = model(x)\n",
    "print(\"-----------\\n\")    \n",
    "print(\"Final output Raw: \",result)\n",
    "print(\"Final output Rounded: \",np.round(result))\n",
    "print(\"-----------\\n\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iter :  0  Current cost:  3.4757114842709105\n",
      "Current iter :  1000  Current cost:  2.65246749668983\n",
      "Current iter :  2000  Current cost:  1.8283242889590927\n",
      "Current iter :  3000  Current cost:  1.0573867076973986\n",
      "Current iter :  4000  Current cost:  0.5169706985650823\n",
      "Current iter :  5000  Current cost:  0.22308238084406407\n",
      "Current iter :  6000  Current cost:  0.09141527443934538\n",
      "Current iter :  7000  Current cost:  0.039537283832087246\n",
      "Current iter :  8000  Current cost:  0.02062295641853802\n",
      "Current iter :  9000  Current cost:  0.014030167784857597\n",
      "Current iter :  10000  Current cost:  0.011790523183310718\n",
      "Current iter :  11000  Current cost:  0.011040770537507417\n",
      "Current iter :  12000  Current cost:  0.010791873351766498\n",
      "Current iter :  13000  Current cost:  0.010709640867891305\n",
      "Current iter :  14000  Current cost:  0.010682546576945779\n",
      "-----------\n",
      "\n",
      "Final output Raw:  [3.06420172 1.8540511  1.08122782]\n",
      "Final output Rounded:  [3. 2. 1.]\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "number_of_epoch = 15000\n",
    "wx, wrec= learn(x,y,number_of_epoch)\n",
    "\n",
    "result = model(x)\n",
    "print(\"-----------\\n\")    \n",
    "print(\"Final output Raw: \",result)\n",
    "print(\"Final output Rounded: \",np.round(result))\n",
    "print(\"-----------\\n\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
