{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The CNN architecture we are using for this tutorial is SmallerVGGNet , a simplified version of itâ€™s big brother, VGGNet . The VGGNet  model was first introduced by Simonyan and Zisserman in their 2014 paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ds/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallerVGGNet:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, finalAct=\"softmax\"):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "        \n",
    "        \t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "        \n",
    "        \t\t# first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(1024))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\t# use a *softmax* activation for single-label classification\n",
    "\t\t# and *sigmoid* activation for multi-label classification\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(finalAct))\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# batch size, and image dimensions\n",
    "INIT_LR = 1e-3\n",
    "IMAGE_DIMS = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "from imutils import paths\n",
    "import random\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = sorted(list(paths.list_images(\"data/image/multi/miniDataset\")))\n",
    "#imagePaths = sorted(list(paths.list_images(\"data/image/screen\")))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# initialize the data and labels\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import os\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, pre-process it, and store it in the data list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract set of class labels from the image path and update the\n",
    "\t# labels list\n",
    "\tl = label = imagePath.split(os.path.sep)[-2].split(\"_\")\n",
    "\tlabels.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data matrix: 297 images (64.15MB)\n",
      "297\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"[INFO] data matrix: {} images ({:.2f}MB)\".format(len(imagePaths), data.nbytes / (1024 * 1000.0)))\n",
    "\n",
    "print(len(labels))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] class labels:\n",
      "1. black\n",
      "2. blue\n",
      "3. dress\n",
      "4. jeans\n",
      "5. red\n",
      "6. shirt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# binarize the labels using scikit-learn's special multi-label\n",
    "# binarizer implementation\n",
    "print(\"[INFO] class labels:\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    "\n",
    "# loop over each of the possible class labels and show them\n",
    "for (i, label) in enumerate(mlb.classes_):print(\"{}. {}\".format(i + 1, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "\thorizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "# initialize the model using a sigmoid activation as the final layer\n",
    "# in the network so we can perform multi-label classification\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = SmallerVGGNet.build(\n",
    "\twidth=IMAGE_DIMS[1], height=IMAGE_DIMS[0],\n",
    "\tdepth=IMAGE_DIMS[2], classes=len(mlb.classes_),\n",
    "\tfinalAct=\"sigmoid\")\n",
    "\n",
    "EPOCHS = 10 #75\n",
    "\n",
    "# initialize the optimizer (SGD is sufficient)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile the model using binary cross-entropy rather than\n",
    "# categorical cross-entropy -- this may seem counterintuitive for\n",
    "# multi-label classification, but keep in mind that the goal here\n",
    "# is to treat each output label as an independent Bernoulli\n",
    "# distribution\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/10"
     ]
    }
   ],
   "source": [
    "\n",
    "BS = 10 #32\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tepochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing network...\n",
      "[INFO] serializing label binarizer...\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(\"tmp/cloth.model\")\n",
    "\n",
    "# save the multi-label binarizer to disk\n",
    "print(\"[INFO] serializing label binarizer...\")\n",
    "f = open(\"tmp/mlb.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(mlb))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading network...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "# load the trained convolutional neural network and the multi-label\n",
    "# binarizer\n",
    "print(\"[INFO] loading network...\")\n",
    "model = load_model(\"tmp/cloth.model\")\n",
    "mlb = pickle.loads(open(\"tmp/mlb.pickle\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import imutils\n",
    "import cv2\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "\n",
    "# load the image\n",
    "image = cv2.imread(\"data/image/multi/examples/example_06.jpg\")\n",
    "#image = cv2.imread(\"data/image/screen/test2.jpg\")\n",
    "output = imutils.resize(image, width=400)\n",
    " \n",
    "# pre-process the image for classification\n",
    "image = cv2.resize(image, (96, 96))\n",
    "image = image.astype(\"float\") / 255.0\n",
    "image = img_to_array(image)\n",
    "image = np.expand_dims(image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] classifying image...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# classify the input image then find the indexes of the two class\n",
    "# labels with the *largest* probability\n",
    "print(\"[INFO] classifying image...\")\n",
    "proba = model.predict(image)[0]\n",
    "idxs = np.argsort(proba)[::-1][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email: 98.68%\n",
      "excel: 99.96%\n",
      "screen: 71.72%\n",
      "word: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# show the probabilities for each of the individual labels\n",
    "for (label, p) in zip(mlb.classes_, proba):\n",
    "\tprint(\"{}: {:.2f}%\".format(label, p * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop over the indexes of the high confidence class labels\n",
    "for (i, j) in enumerate(idxs):\n",
    "\t# build the label and draw the label on the image\n",
    "\tlabel = \"{}: {:.2f}%\".format(mlb.classes_[j], proba[j] * 100)\n",
    "\tcv2.putText(output, label, (10, (i * 30) + 25), \n",
    "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "# show the probabilities for each of the individual labels\n",
    "for (label, p) in zip(mlb.classes_, proba):\n",
    "\tprint(\"{}: {:.2f}%\".format(label, p * 100))\n",
    "\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", output)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
